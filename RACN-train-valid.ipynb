{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "TRAINING = True\n",
    "tf.reset_default_graph()\n",
    "def identity_block(X_input, kernel_size, filters, stage, block, relu):\n",
    "    # defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    with tf.name_scope(\"id_block_stage\"+str(stage)):\n",
    "        filter1, filter2, filter3 = filters\n",
    "        X_shortcut = X_input\n",
    "\n",
    "        # First component of main path\n",
    "        x = tf.layers.conv2d(X_input, filter1,\n",
    "                 kernel_size=(1, 1), strides=(1, 1),name=conv_name_base+'2a')\n",
    "        x = tf.layers.batch_normalization(x, axis=3, name=bn_name_base+'2a', training=TRAINING)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        # Second component of main path\n",
    "        x = tf.layers.conv2d(x, filter2, (kernel_size, kernel_size),\n",
    "                                 padding='same', name=conv_name_base+'2b')\n",
    "        x = batch_norm2 = tf.layers.batch_normalization(x, axis=3, name=bn_name_base+'2b', training=TRAINING)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        # Third component of main path\n",
    "        x = tf.layers.conv2d(x, filter3, kernel_size=(1, 1),name=conv_name_base+'2c')\n",
    "        x = tf.layers.batch_normalization(x, axis=3, name=bn_name_base + '2c', training=TRAINING)\n",
    "\n",
    "        # Final step: Add shortcut value to main path, and pass it through a RELU activation\n",
    "        X_add_shortcut = tf.add(x, X_shortcut)\n",
    "        if relu==True:\n",
    "            add_result = tf.nn.relu(X_add_shortcut)\n",
    "        else:\n",
    "            add_result=X_add_shortcut\n",
    "    return add_result\n",
    "\n",
    "def convolutional_block(X_input, kernel_size, filters, stage, block, stride, relu):\n",
    "    #change the shape of output so that it can do sum process with shotcut\n",
    "    \n",
    "    # defining name basis\n",
    "    conv_name_base = 'res' + str(stage) + block + '_branch'\n",
    "    bn_name_base = 'bn' + str(stage) + block + '_branch'\n",
    "\n",
    "    with tf.name_scope(\"conv_block_stage\" + str(stage)):\n",
    "\n",
    "        # Retrieve Filters\n",
    "        filter1, filter2, filter3 = filters\n",
    "\n",
    "        # Save the input value\n",
    "        X_shortcut = X_input\n",
    "\n",
    "        # First component of main path\n",
    "        x = tf.layers.conv2d(X_input, filter1,\n",
    "                                 kernel_size=(1, 1),\n",
    "                                 strides=(stride, stride),\n",
    "                                 name=conv_name_base+'2a')\n",
    "        x = tf.layers.batch_normalization(x, axis=3, name=bn_name_base+'2a', training=TRAINING)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        # Second component of main path\n",
    "        x = tf.layers.conv2d(x, filter2, (kernel_size, kernel_size), name=conv_name_base + '2b',padding='same')\n",
    "        x = tf.layers.batch_normalization(x, axis=3, name=bn_name_base + '2b', training=TRAINING)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        # Third component of main path\n",
    "        x = tf.layers.conv2d(x, filter3, (1, 1), name=conv_name_base + '2c')\n",
    "        x = tf.layers.batch_normalization(x, axis=3, name=bn_name_base + '2c', training=TRAINING)\n",
    "\n",
    "        # SHORTCUT PATH\n",
    "        X_shortcut = tf.layers.conv2d(X_shortcut, filter3, (1,1),\n",
    "                                      strides=(stride, stride), name=conv_name_base + '1')\n",
    "        X_shortcut = tf.layers.batch_normalization(X_shortcut, axis=3, name=bn_name_base + '1', training=TRAINING)\n",
    "\n",
    "        # Final step: Add shortcut value to main path, and pass it through a RELU activation\n",
    "        X_add_shortcut = tf.add(X_shortcut, x)\n",
    "        if relu==True:\n",
    "            add_result = tf.nn.relu(X_add_shortcut)\n",
    "        else:\n",
    "            add_result = X_add_shortcut\n",
    "    return add_result\n",
    "\n",
    "def attention_block(X_input, channel_output, attention_name, p=1, t=2, r=1):\n",
    "    def upsample(x_t, name, size):\n",
    "        with tf.name_scope(name):\n",
    "            outputs = tf.image.resize_bilinear(x_t, size)\n",
    "            return outputs\n",
    "        \n",
    "    with tf.name_scope(attention_name+'pre_processing'):\n",
    "        pre_pros = convolutional_block(X_input, 3, [channel_output/4, channel_output/4, channel_output], \n",
    "                                       stage=attention_name, block='pre_proc', stride=1, relu=False)\n",
    "    \n",
    "    with tf.name_scope(attention_name+'trunk_branch'):\n",
    "        for idx in range(t):\n",
    "            idx=str(idx)\n",
    "            trunks = convolutional_block(pre_pros,3, [channel_output/4, channel_output/4, channel_output], \n",
    "                                       stage=attention_name, block='trunk'+idx, stride=1, relu=False)\n",
    "    with tf.name_scope(attention_name+'mask_branch'):\n",
    "        size_1 = pre_pros.get_shape().as_list()[1:3]\n",
    "        mask_1 = tf.layers.max_pooling2d(pre_pros, pool_size=(3, 3),strides=(2, 2),name='pool_mask_1')\n",
    "        for idx in range(r):\n",
    "            idx=str(idx)\n",
    "            mask_1 = convolutional_block(mask_1, 3, [channel_output/4, channel_output/4, channel_output], \n",
    "                                       stage=attention_name, block='mask_1'+idx, stride=1, relu=False)\n",
    "        size_2 = mask_1.get_shape().as_list()[1:3]\n",
    "        mask_2 = tf.layers.max_pooling2d(mask_1, pool_size=(3, 3),strides=(2, 2),name='pool_mask_2')\n",
    "        for idx in range(2*r):\n",
    "            idx=str(idx)\n",
    "            mask_2 = convolutional_block(mask_2, 3, [channel_output/4, channel_output/4, channel_output], \n",
    "                                       stage=attention_name, block='mask_2'+idx, stride=1, relu=False)\n",
    "        mask_3 =  upsample(mask_2,'upsample_1',size_2)\n",
    "        mask_3 = tf.add(mask_3, mask_1, name=\"fuse_add\")\n",
    "        for idx in range(r):\n",
    "            idx=str(idx)\n",
    "            mask_3 = convolutional_block(mask_3, 3, [channel_output/4, channel_output/4, channel_output], \n",
    "                                       stage=attention_name, block='mask_3'+idx, stride=1, relu=False)\n",
    "        mask_4 = upsample(mask_3,'upsample_2',size_1)\n",
    "        mask_4 = convolutional_block(mask_4, 3, [channel_output/4, channel_output/4, channel_output], \n",
    "                                       stage=attention_name, block='mask_4', stride=1, relu=True)\n",
    "        mask_4 = tf.nn.sigmoid(mask_4, \"mask_sigmoid\")\n",
    "    with tf.name_scope(attention_name+'fusing'):\n",
    "        x = tf.multiply(trunks, mask_4, name='fus_mul')\n",
    "        x = tf.add(x, trunks, name='fus_add')\n",
    "    with tf.name_scope(attention_name+'post_processing'):\n",
    "        for idx in range(p):\n",
    "            idx=str(idx)\n",
    "            output = convolutional_block(x, 3, [channel_output/4, channel_output/4, channel_output], \n",
    "                                       stage=attention_name, block='post_pros', stride=1, relu=True)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_attention_inference(X_input, class_num=2):\n",
    "    X = X_input\n",
    "    x = tf.layers.conv2d(X, filters=32, kernel_size=(3, 3), strides=(1, 1), name='pre_cov')\n",
    "    #64\n",
    "    x = tf.layers.batch_normalization(x, axis=3, name='pre_bn_conv1')\n",
    "    x = attention_block(x, 32, 'attention_1')\n",
    "    #64\n",
    "    x = convolutional_block(x, 3, [8,8,32],stage='trans', block='1', stride=1, relu=False)\n",
    "    #64\n",
    "    x = attention_block(x, 32, 'attention_2')\n",
    "    #64\n",
    "#     x = convolutional_block(x, 3, [8,8,32],stage='trans', block='2', stride=1, relu=False)\n",
    "#     #64\n",
    "#     x = attention_block(x, 32, 'attention_3')\n",
    "    #64\n",
    "    x = convolutional_block(x, 3, [64, 64, 256],stage='pos_pros', block='1', stride=2, relu=False)\n",
    "    #32\n",
    "#     x = convolutional_block(x, kernel_size=3, filters=[64, 64, 256], stage='resnet1', block='a', stride=2, relu=True)\n",
    "#     x = identity_block(x, 3, [64, 64, 256], stage='resnet1', block='b', relu=True)\n",
    "#     x = identity_block(x, 3, [64, 64, 256], stage='resnet1', block='c', relu=True)\n",
    "    #16\n",
    "    x = convolutional_block(x, kernel_size=3, filters=[256, 256, 1024], stage='resnet2', block='a', stride=1, relu=True)\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage='resnet2', block='b', relu=True)\n",
    "    x = identity_block(x, 3, [256, 256, 1024], stage='resnet2', block='c', relu=True)\n",
    "#     x = identity_block(x, 3, [256, 256, 1024], stage='resnet2', block='d', relu=True)\n",
    "#     x = identity_block(x, 3, [256, 256, 1024], stage='resnet2', block='e', relu=True)\n",
    "#     x = identity_block(x, 3, [256, 256, 1024], stage='resnet2', block='f', relu=True)\n",
    "    #16\n",
    "    #x = tf.nn.lrn(x, depth_radius=4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2')\n",
    "    x = tf.layers.average_pooling2d(x, pool_size=(2, 2), strides=(1,1))\n",
    "    flatten = tf.layers.flatten(x, name='flatten')\n",
    "    keep_prob = 0.5\n",
    "    # dropout\n",
    "    drop = tf.nn.dropout(flatten, keep_prob)\n",
    "    dense1 = tf.layers.dense(drop, units=50, activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(dense1, units=2, activation=tf.nn.softmax)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def losses(logits, labels, name):\n",
    "    with tf.variable_scope('loss') as scope:\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits \\\n",
    "            (logits=logits, labels=labels, name='xentropy_per_example')\n",
    "        loss = tf.reduce_mean(cross_entropy, name='loss')\n",
    "        if name=='train':\n",
    "            tf.summary.scalar(scope.name + '/train_loss', loss)\n",
    "        if name=='valid':\n",
    "            tf.summary.scalar(scope.name + '/valid_loss', loss)\n",
    "    return loss\n",
    " \n",
    "def trainning(loss, learning_rate):\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate= learning_rate)\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        train_op = optimizer.minimize(loss, global_step= global_step)\n",
    "    return train_op\n",
    " \n",
    "def evaluation(logits, labels, name):\n",
    "    with tf.variable_scope('accuracy') as scope:\n",
    "        correct = tf.nn.in_top_k(logits, labels, 1)\n",
    "        correct = tf.cast(correct, tf.float16)\n",
    "        accuracy = tf.reduce_mean(correct)\n",
    "        if name=='train':\n",
    "            tf.summary.scalar(scope.name + '/train_accuracy', accuracy)\n",
    "        if name=='valid':\n",
    "            tf.summary.scalar(scope.name + '/valid_accuracy', accuracy)\n",
    "    return accuracy\n",
    "\n",
    "def recall_precision(logits, labels, name):\n",
    "    logits = tf.cast(logits, tf.int64)\n",
    "    labels = tf.cast(labels, tf.int64)\n",
    "    predict = tf.arg_max(logits,1)\n",
    "    with tf.variable_scope('recall_precision') as scope:\n",
    "        TP = tf.count_nonzero(predict * labels)\n",
    "        TN = tf.count_nonzero((predict - 1) * (labels - 1))\n",
    "        FN = tf.count_nonzero(predict * (labels - 1))\n",
    "        FP = tf.count_nonzero((predict - 1) * labels)\n",
    "        precision = tf.divide(TP, TP + FP)\n",
    "        recall = tf.divide(TP, TP + FN)\n",
    "        precision = tf.cast(precision, dtype=tf.float64)\n",
    "        recall = tf.cast(recall, dtype=tf.float64)\n",
    "        #f1 = 2 * precision * recall / (precision + recall)\n",
    "        #f1 = tf.cast(f1, dtype=tf.float32)\n",
    "        if name=='train':\n",
    "            tf.summary.scalar(scope.name + '/train_precision', precision)\n",
    "            tf.summary.scalar(scope.name + '/train_recall', recall)\n",
    "        if name=='valid':\n",
    "            tf.summary.scalar(scope.name + '/valid_precision', precision)\n",
    "            tf.summary.scalar(scope.name + '/valid_recall', recall)\n",
    "    return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "def get_files(path_pos,path_neg,label_pos,label_neg):\n",
    "    TC = []\n",
    "    label_TC = []\n",
    "    nonTC = []\n",
    "    label_nonTC = []\n",
    "    # data loader\n",
    "    file_dir_TC=path_pos\n",
    "    file_dir_nonTC=path_neg\n",
    "    TC_list = os.listdir(file_dir_TC)\n",
    "    nonTC_list = os.listdir(file_dir_nonTC)\n",
    "    for file in TC_list[:len(nonTC_list)]:\n",
    "        name = file.split('_')\n",
    "        if name[0] == label_pos:\n",
    "            TC.append(file_dir_TC + file)\n",
    "            label_TC.append(1)\n",
    "    for file in nonTC_list:\n",
    "        name = file.split('_')\n",
    "        if name[0] == label_neg:\n",
    "            nonTC.append(file_dir_nonTC + file)\n",
    "            label_nonTC.append(0)\n",
    "    print(\"There are %d TC\\nThere are %d nonTC\" % (len(TC), len(nonTC)))\n",
    " \n",
    "    # shuffle\n",
    "    image_list = np.hstack((TC, nonTC))\n",
    "    label_list = np.hstack((label_TC, label_nonTC))\n",
    "    temp = np.array([image_list, label_list])\n",
    "    temp = temp.transpose()    \n",
    "    np.random.shuffle(temp)\n",
    " \n",
    "    image_list = list(temp[:, 0])\n",
    "    label_list = list(temp[:, 1])\n",
    "    label_list = [int(i) for i in label_list]\n",
    " \n",
    "    return image_list, label_list\n",
    " \n",
    "# img_list,label_list = get_files(file_dir)\n",
    " \n",
    "# batch\n",
    "def get_batch(image, label, image_W, image_H, batch_size, capacity):   \n",
    "    image = tf.cast(image, tf.string)\n",
    "    label = tf.cast(label, tf.int32)\n",
    " \n",
    "    # queue\n",
    "    input_queue = tf.train.slice_input_producer([image, label])\n",
    " \n",
    "    image_contents = tf.read_file(input_queue[0])\n",
    "    label = input_queue[1]\n",
    "    image = tf.image.decode_jpeg(image_contents, channels=1)\n",
    " \n",
    "    # resize\n",
    "    image = tf.image.resize_images(image, [image_H, image_W], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    # image = tf.image.per_image_standardization(image)  \n",
    "    image_batch, label_batch = tf.train.batch([image, label],\n",
    "                                              batch_size=batch_size,\n",
    "                                              num_threads=64,  \n",
    "                                              capacity=capacity)\n",
    "  \n",
    "    return image_batch, label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1737956 TC\n",
      "There are 1737956 nonTC\n",
      "There are 14355 TC\n",
      "There are 434488 nonTC\n",
      "WARNING:tensorflow:From <ipython-input-4-0053a92f2987>:46: slice_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(tuple(tensor_list)).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/input.py:372: range_input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.range(limit).shuffle(limit).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/input.py:318: input_producer (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensor_slices(input_tensor).shuffle(tf.shape(input_tensor, out_type=tf.int64)[0]).repeat(num_epochs)`. If `shuffle=False`, omit the `.shuffle(...)`.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/input.py:188: limit_epochs (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.from_tensors(tensor).repeat(num_epochs)`.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/input.py:197: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow/python/training/input.py:197: add_queue_runner (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "WARNING:tensorflow:From <ipython-input-4-0053a92f2987>:59: batch (from tensorflow.python.training.input) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Queue-based input pipelines have been replaced by `tf.data`. Use `tf.data.Dataset.batch(batch_size)` (or `padded_batch(...)` if `dynamic_pad=True`).\n",
      "WARNING:tensorflow:From <ipython-input-3-b39e5d5309b2>:32: arg_max (from tensorflow.python.ops.gen_math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `argmax` instead\n",
      "WARNING:tensorflow:From <ipython-input-5-08ce9b915272>:62: start_queue_runners (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "----------training start---------\n",
      "Step 0, train loss = 0.75, train accuracy = 51.56%, train recall = 0.00%, train precision = nan%\n",
      "Step 50, train loss = 0.81, train accuracy = 50.00%, train recall = 100.00%, train precision = 50.00%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "N_CLASSES = 2 \n",
    "IMG_W = 64  # resize\n",
    "IMG_H = 64\n",
    "BATCH_SIZE = 64\n",
    "CAPACITY = 2000\n",
    "MAX_STEP = 20000000 \n",
    "learning_rate = 0.0005 \n",
    " \n",
    "train_dir = './train_attention_resnet_Residual/'\n",
    "logs_train_dir = './train_attention_resnet_Residual/'\n",
    "\n",
    "file_dir_TC='/home/ubuntu/data/TC/'\n",
    "file_dir_nonTC='/home/ubuntu/data/nonTC/'\n",
    "file_dir_valTC='/home/ubuntu/data/valTC_cp/'\n",
    "file_dir_valnonTC='/home/ubuntu/data/valnonTC/'\n",
    "\n",
    "train, train_label = get_files(file_dir_TC,file_dir_nonTC,'TC','nonTC')\n",
    "valid, valid_label = get_files(file_dir_valTC,file_dir_valnonTC,'valTC','valnonTC')\n",
    "train_batch_op,train_label_batch_op=get_batch(train,\n",
    "                                train_label,\n",
    "                                IMG_W,\n",
    "                                IMG_H,\n",
    "                                BATCH_SIZE,\n",
    "                                CAPACITY)\n",
    "\n",
    "valid_batch_op,valid_label_batch_op=get_batch(valid,\n",
    "                                valid_label,\n",
    "                                IMG_W,\n",
    "                                IMG_H,\n",
    "                                BATCH_SIZE,\n",
    "                                CAPACITY)\n",
    "x = tf.placeholder(tf.float32, [BATCH_SIZE,IMG_W,IMG_H,1])\n",
    "y = tf.placeholder(tf.float32, [BATCH_SIZE])\n",
    "y = tf.cast(y,tf.int64)\n",
    "\n",
    "train_logits = res_attention_inference(x)\n",
    "train_loss = losses(train_logits, y, 'train')\n",
    "train_op = trainning(train_loss, learning_rate)\n",
    "train_acc = evaluation(train_logits, y, 'train')\n",
    "train_recall, train_precision = recall_precision(train_logits, y, 'train')\n",
    "\n",
    "valid_loss = losses(train_logits, y, 'valid')\n",
    "valid_acc = evaluation(train_logits, y, 'valid')\n",
    "valid_recall, valid_precision = recall_precision(train_logits, y, 'valid')\n",
    "\n",
    "summary_op = tf.summary.merge_all() \n",
    " \n",
    "sess = tf.Session()\n",
    "\n",
    "train_writer = tf.summary.FileWriter(logs_train_dir, sess.graph)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    " \n",
    "\n",
    "coord = tf.train.Coordinator()\n",
    "threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    " \n",
    "try:\n",
    "    print('----------training start---------')\n",
    "    train_result=[]\n",
    "    valid_result=[]\n",
    "    for step in np.arange(MAX_STEP):\n",
    "        train_batch,train_label_batch = sess.run([train_batch_op,train_label_batch_op])\n",
    "        if coord.should_stop():\n",
    "                break\n",
    "       \n",
    "        _, tra_loss, tra_acc, tra_recall, tra_precision= sess.run([train_op, train_loss, train_acc, train_recall, train_precision],feed_dict={x:train_batch,y:train_label_batch})\n",
    "        summary_str = sess.run(summary_op,feed_dict={x:train_batch,y:train_label_batch})\n",
    "        train_writer.add_summary(summary_str, step)\n",
    "        train_result.append([step, tra_loss, tra_acc*100.0, tra_recall*100.0, tra_precision*100.0])\n",
    "        if step % 50 == 0:\n",
    "            print('Step %d, train loss = %.2f, train accuracy = %.2f%%, train recall = %.2f%%, train precision = %.2f%%' %(step, tra_loss, tra_acc*100.0, tra_recall*100.0, tra_precision*100.0))\n",
    "            \n",
    "            summary = tf.Summary()\n",
    "            summary.value.add(tag='tra_loss', simple_value=tra_loss)\n",
    "            summary.value.add(tag='tra_acc', simple_value=tra_acc)\n",
    "            summary.value.add(tag='tra_recall', simple_value=tra_recall)\n",
    "            summary.value.add(tag='tra_precision', simple_value=tra_precision)\n",
    "            train_writer.add_summary(summary, step)\n",
    "        if step % 500 == 0:\n",
    "            ckpt=tf.train.get_checkpoint_state('./train_attention_resnet_Residual/')\n",
    "#             print(ckpt)\n",
    "            if ckpt and ckpt.all_model_checkpoint_paths:\n",
    "                valid_batch,valid_label_batch = sess.run([valid_batch_op,valid_label_batch_op])\n",
    "                saver.restore(sess,ckpt.model_checkpoint_path)\n",
    "                val_loss, val_acc, val_recall, val_precision = sess.run([valid_loss, valid_acc, valid_recall, valid_precision], feed_dict={x:valid_batch,y:valid_label_batch})\n",
    "                valid_result.append([step, val_loss, val_acc*100.0, val_recall*100.0, val_precision*100.0])\n",
    "                print('*********************')\n",
    "                print('Step %d, valid loss = %.2f, valid accuracy = %.2f%%, valid recall = %.2f%%, valid precision = %.2f%%' %(step, val_loss, val_acc*100.0, val_recall*100.0, val_precision*100.0))\n",
    "                print('*********************')\n",
    "                summary = tf.Summary()\n",
    "                summary.value.add(tag='val_loss', simple_value=val_loss)\n",
    "                summary.value.add(tag='val_acc', simple_value=val_acc)\n",
    "                summary.value.add(tag='val_recall', simple_value=val_recall)\n",
    "                summary.value.add(tag='val_precision', simple_value=val_precision)\n",
    "                train_writer.add_summary(summary, step)\n",
    "#                 summary_str = sess.run(summary_op,feed_dict={x:valid_batch,y:valid_label_batch})\n",
    "#                 train_writer.add_summary(summary_str, step)\n",
    "            \n",
    "      \n",
    "        if step % 500 == 0 or (step + 1) == MAX_STEP:\n",
    "            checkpoint_path = os.path.join(logs_train_dir, 'model.ckpt')\n",
    "            saver.save(sess, checkpoint_path, global_step=step)\n",
    "\n",
    "except tf.errors.OutOfRangeError:\n",
    "    print('Done training -- epoch limit reached')\n",
    "\n",
    "finally:\n",
    "    coord.request_stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
